{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e716cd-5d42-4816-a666-279541aca3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from predibase import PredibaseClient\n",
    "\n",
    "from ludwig.data.split import get_splitter\n",
    "from ludwig.data.negative_sampling import negative_sample\n",
    "from ludwig.backend.base import LocalBackend\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbe747-5965-4748-afd6-e0ea3a1e06d8",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "# Personalized Email Campaings 📨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025b901-6650-4745-9262-cc7e3876ce41",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Recommender System Model --> LLM Generated Personalized Email Campaigns__\n",
    "\n",
    "In this tutorial, we show how to use the Predibase SDK to train a recommender system (recsys) model, then take the results from the recsys model and generate personalized outreach emails via generations from open-source LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c1dff-6499-4c5c-a637-2faab6fd2c87",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Authentication 🔐\n",
    "\n",
    "The first step is to sign into Predibase.\n",
    "\n",
    "- If you do not have a Predibase account set up yet, you may sign up for a free account [here](https://predibase.com/free-trial)\n",
    "- If you already have an account, navigate to Settings -> My Profile and generate a new API token.\n",
    "- Finally, plug in the generated API token in the code below to authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316e586-a8d2-4b3a-b13f-385971ea0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PredibaseClient(\n",
    "    token=\"YOUR TOKEN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5feb6e-7e11-48a8-9d68-bd7f80c5de59",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "## Dataset Preparation 📄\n",
    "\n",
    "Next we'll prepare the dataset needed to train the model the recsys model. For this demonstration, we will be using the [H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data)  dataset from Kaggle. For this tutorial, we do not need the images, which are the largest part of the download, so we recommend individually downloading `articles.csv`, `customers.csv`, and `transactions_train.csv` to minimize the download process time. This dataset contains the purchase history of customers across time, along with supporting metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694a76c-b7e3-4186-a98a-bac494c97d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(\"PATH TO articles.csv DOWNLOAD\")\n",
    "customers_df = pd.read_csv(\"PATH TO customers.csv DOWNLOAD\")\n",
    "transactions_df = pd.read_csv(\"PATH TO transactions_train.csv DOWNLOAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f44f2-8252-4639-8e00-de0f1bb82b47",
   "metadata": {},
   "source": [
    "<br/>\n",
    "First we're going to merge the three separate dataframes together to form a single transaction dataframe. This dataframe will contain all transactions that took place by the users in the dataset. Because every row is a transaction that took place, we need to add in some examples of transactions that didn't take place - we call these negative samples. We will handle that a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1c26c-a6ab-462d-8e67-e8bf4e879163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the transactions and articles dataframes\n",
    "transactions_df = pd.merge(\n",
    "    transactions_df,\n",
    "    articles_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"article_id\",\n",
    "    right_on=\"article_id\",\n",
    ")\n",
    "\n",
    "# Merge the transactions and customers dataframes\n",
    "transactions_df = pd.merge(\n",
    "    transactions_df,\n",
    "    customers_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"customer_id\",\n",
    "    right_on=\"customer_id\",\n",
    ")\n",
    "\n",
    "# Set label to 1 for all known transactions, since the customer bought the article\n",
    "transactions_df[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829522ba-447b-40cb-84a0-e4fb3ee66fa1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Now that we have a single dataframe with all transaction, article, and customer data present in every row, we are going to both sample and split the dataset. \n",
    "\n",
    "#### Sampling\n",
    "We sample the dataset here because of scale. The `transactions_df` has ~31.8 million rows and ~1.3 million unique customers. This is great for training a very strong model, however, processing this data will take considerably longer. For the purposes of this tutorial, we are sampling in the following ways:\n",
    "- Sample from transactions that occured after August 21st, 2020\n",
    "- Sample from customers who purchased 10 or more items after August 21st, 2020\n",
    "- Select 500 samples\n",
    "\n",
    "\n",
    "#### Split\n",
    "We need to split our data into a training, validation, and test set. In addition to this though, we also need to split in a way that makes sure a given customer's transaction are present in each set. This way, the learning that takes place in the training set can be fairly evaluated in the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f25b6e-9a96-4a45-8272-68ba4bb50107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transaction date column to datetime object\n",
    "transactions_df[\"t_dat\"] = pd.to_datetime(transactions_df.t_dat)\n",
    "\n",
    "# Strip out year and month for splitting/sampling purposes\n",
    "transactions_df[\"year_month\"] = transactions_df.t_dat.dt.to_period(\"M\").dt.strftime(\"%Y-%m\")\n",
    "\n",
    "# Slice dataset to keep everything after \"2020-08-21\"\n",
    "sampled_transactions_df = transactions_df[transactions_df.t_dat > \"2020-08-21\"]\n",
    "\n",
    "# Sample 500 customers with 10 or more transactions, since ~1.3 million total customers takes quite a while to process\n",
    "customer_ids = np.random.choice(sampled_transactions_df.groupby(\"customer_id\").filter(lambda x: len(x) >= 10).customer_id, 500, replace=False)\n",
    "sampled_transactions_df = sampled_transactions_df[sampled_transactions_df.customer_id.isin(customer_ids)]\n",
    "\n",
    "# Get ludwig datetime splitter to ensure no target leakage by date. Split 70%, 20%, 10% for train, validation, and test sets\n",
    "splitter = get_splitter(\"datetime\", column=\"year_month\", probabilities=(0.7, 0.2, 0.1))\n",
    "\n",
    "# Split per customer_id to ensure that interactions for a customer are across all splits\n",
    "train_dfs, val_dfs, test_dfs = [], [], []\n",
    "for customer_id in sampled_transactions_df[\"customer_id\"].unique():\n",
    "    train_df, val_df, test_df = splitter.split(sampled_transactions_df[sampled_transactions_df[\"customer_id\"] == customer_id], backend=LocalBackend())\n",
    "    \n",
    "    train_dfs.append(train_df)\n",
    "    val_dfs.append(val_df)\n",
    "    test_dfs.append(test_df)\n",
    "    \n",
    "# Concatenate all customer id specific splits into their respective datasets\n",
    "train_set = pd.concat(train_dfs)\n",
    "val_set = pd.concat(val_dfs)\n",
    "test_set = pd.concat(test_dfs)\n",
    "\n",
    "# Set the split value for each set\n",
    "train_set[\"split\"] = 0\n",
    "val_set[\"split\"] = 1\n",
    "test_set[\"split\"] = 2\n",
    "\n",
    "# Combine train, val, and test set into final dataset\n",
    "full_df = pd.concat([train_set, val_set, test_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17797d3-c7d8-48bc-a4bf-128feedaeac7",
   "metadata": {},
   "source": [
    "<br/>\n",
    "As mentioned before, we need to add negative samples so that the model can learn to distinguish between something a user would buy, and something a user would not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86debc3-b33d-4772-b857-64b88dcf6060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sample each split separately\n",
    "train_df = negative_sample(full_df[full_df.split == 0], neg_pos_ratio=10, neg_val=0)\n",
    "val_df = negative_sample(full_df[full_df.split == 1], neg_pos_ratio=10, neg_val=0)\n",
    "test_df = negative_sample(full_df[full_df.split == 2], neg_pos_ratio=10, neg_val=0)\n",
    "\n",
    "# The negative_sample utility from Ludwig only returns a user_id, item_id, and label column. So\n",
    "# we need to define the columns we want to add back into the dataset.\n",
    "article_cols = [\n",
    "    \"prod_name\",\n",
    "    \"product_type_name\",\n",
    "    \"product_group_name\",\n",
    "    \"graphical_appearance_name\",\n",
    "    \"colour_group_name\",\n",
    "    \"perceived_colour_value_name\",\n",
    "    \"perceived_colour_master_name\",\n",
    "    \"department_name\",\n",
    "    \"index_name\",\n",
    "    \"index_group_name\",\n",
    "    \"section_name\",\n",
    "    \"garment_group_name\",\n",
    "    \"detail_desc\",\n",
    "]\n",
    "customer_cols = [\n",
    "    \"customer_id\",\n",
    "    \"FN\",\n",
    "    \"Active\",\n",
    "    \"club_member_status\",\n",
    "    \"fashion_news_frequency\",\n",
    "    \"age\",\n",
    "    \"postal_code\",\n",
    "]\n",
    "\n",
    "# Add back customer and article features defined above\n",
    "articles = full_df[[\"article_id\"] + article_cols].drop_duplicates([\"article_id\"])\n",
    "customers = full_df[customer_cols].drop_duplicates([\"customer_id\"])\n",
    "\n",
    "train_df = pd.merge(train_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
    "train_df = pd.merge(train_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
    "train_df[\"split\"] = 0\n",
    "\n",
    "val_df = pd.merge(val_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
    "val_df = pd.merge(val_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
    "val_df[\"split\"] = 1\n",
    "\n",
    "test_df = pd.merge(test_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
    "test_df = pd.merge(test_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
    "test_df[\"split\"] = 2\n",
    "\n",
    "# Combine train, val, and test sets together to get the final dataset we will train the model with.\n",
    "final_df = pd.concat([train_df, val_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b1f0e-919a-4f4b-8e30-ffb10eb12360",
   "metadata": {},
   "source": [
    "<br/> \n",
    "\n",
    "__NOTE:__ Because we decided to sample the dataset to a fraction of it's size, the file size of our dataset is under the 1GB limit that Predibase has on file uploads. So for the purposes of this example, we will go straight from a dataframe to a Predibase dataset. However, for production use cases, it is recommended that you use an object storage such as AWS S3 to hold your dataset artifacts. This way you can connect and train on datasets much larger than 1GB in Predibase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b108ea-85ca-4bb3-bde1-25a1a59addd9",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Connect Data ♾️\n",
    "\n",
    "Here we are using the [`create_dataset_from_df`](https://docs.staging.predibase.com/sdk-guide/datasets/dataset_from_df) method which creates a converts a pandas dataframe to a Predibase File Upload Dataset. It is likely that you are using another connection option and should use a different method of creating a Predibase Dataset. We have all the [connection](https://docs.staging.predibase.com/sdk-guide/connections/) and [dataset](https://docs.staging.predibase.com/sdk-guide/datasets/) options available to reference in the [SDK docs](https://docs.staging.predibase.com/sdk-guide/getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5be08-186f-4bd2-9da3-93251d6e8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dataset to dataframe SDK method to create a Predibase dataset from our final df above.\n",
    "HM_dataset = pc.create_dataset_from_df(final_df, \"H&M_Recsys_Dataset\")\n",
    "HM_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55683865-5518-46ad-b076-59d2b4869ef3",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Engine 🚂\n",
    "\n",
    "At Predibase, engines are our solution to common compute and infrastructure pain points that everyone runs into while training models. These are problems like:\n",
    "- Encountering Out of Memory errors due to insufficient compute\n",
    "- Challenges distributing a model training job over multiple compute resources\n",
    "- Losing progress when transient issues interrupt the training process\n",
    "\n",
    "Predibase training engines mitigate these issues by:\n",
    "- Analyzing the training job details to assign the right amount of compute\n",
    "- Logic to distribute the training job over the assigned compute resources\n",
    "- Retry logic when things go wrong\n",
    "\n",
    "With this in mind, we will select the engine we want to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04d9f3-db89-40f7-aec9-6b85c995b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_engine = pc.get_engine(\"train_engine\")\n",
    "train_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b51795-764d-4524-83a6-2402728eb4d2",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Config 📝\n",
    "\n",
    "Next, we're going to define the config with the specs for our recommender systems model. Here is a readable yaml representation of the config - below I will explain the key parameters we're setting and why.\n",
    "```\n",
    "model_type: ecd\n",
    "input_features:\n",
    "  - name: prod_name\n",
    "    type: text\n",
    "  - name: product_type_name\n",
    "    type: category\n",
    "  - name: product_group_name\n",
    "    type: category\n",
    "  - name: graphical_appearance_name\n",
    "    type: category\n",
    "  - name: colour_group_name\n",
    "    type: category\n",
    "  - name: perceived_colour_value_name\n",
    "    type: category\n",
    "  - name: perceived_colour_master_name\n",
    "    type: category\n",
    "  - name: department_name\n",
    "    type: category\n",
    "  - name: index_name\n",
    "    type: category\n",
    "  - name: index_group_name\n",
    "    type: category\n",
    "  - name: section_name\n",
    "    type: category\n",
    "  - name: garment_group_name\n",
    "    type: category\n",
    "  - name: detail_desc\n",
    "    type: text\n",
    "  - name: customer_id\n",
    "    type: category\n",
    "  - name: FN\n",
    "    type: category\n",
    "  - name: Active\n",
    "    type: category\n",
    "  - name: club_member_status\n",
    "    type: category\n",
    "  - name: fashion_news_frequency\n",
    "    type: category\n",
    "  - name: age\n",
    "    type: number\n",
    "  - name: postal_code\n",
    "    type: category\n",
    "output_features:\n",
    "  - name: label\n",
    "    type: binary\n",
    "    calibration: true\n",
    "preprocessing:\n",
    "  split:\n",
    "    type: fixed\n",
    "combiner:\n",
    "  type: comparator\n",
    "  entity_1:\n",
    "    - prod_name\n",
    "    - product_type_name\n",
    "    - product_group_name\n",
    "    - graphical_appearance_name\n",
    "    - colour_group_name\n",
    "    - perceived_colour_value_name\n",
    "    - perceived_colour_master_name\n",
    "    - department_name\n",
    "    - index_name\n",
    "    - index_group_name\n",
    "    - section_name\n",
    "    - garment_group_name\n",
    "    - detail_desc\n",
    "  entity_2:\n",
    "    - customer_id\n",
    "    - FN\n",
    "    - Active\n",
    "    - club_member_status\n",
    "    - fashion_news_frequency\n",
    "    - age\n",
    "    - postal_code\n",
    "trainer:\n",
    "  epochs: 30\n",
    "  batch_size: 1024\n",
    "  early_stop: 30\n",
    "```\n",
    "A few of the key parameters set are outlined below:\n",
    "- `output_features.calibration`: \n",
    "- `preprocessing.split.type`:\n",
    "- `combiner.type`:\n",
    "- `combiner.entity_1`:\n",
    "- `combiner.entity_2`:\n",
    "\n",
    "For more configuration details, check out the [Ludwig LLM Docs](https://ludwig.ai/0.8/configuration/large_language_model/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcabd4-c7b9-42f5-bbca-96523e839437",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_config = {\n",
    "    'model_type': 'ecd',\n",
    "    'input_features': [\n",
    "        {'name': 'prod_name', 'type': 'text'},\n",
    "        {'name': 'product_type_name', 'type': 'category'},\n",
    "        {'name': 'product_group_name', 'type': 'category'},\n",
    "        {'name': 'graphical_appearance_name', 'type': 'category'},\n",
    "        {'name': 'colour_group_name', 'type': 'category'},\n",
    "        {'name': 'perceived_colour_value_name', 'type': 'category'},\n",
    "        {'name': 'perceived_colour_master_name', 'type': 'category'},\n",
    "        {'name': 'department_name', 'type': 'category'},\n",
    "        {'name': 'index_name', 'type': 'category'},\n",
    "        {'name': 'index_group_name', 'type': 'category'},\n",
    "        {'name': 'section_name', 'type': 'category'},\n",
    "        {'name': 'garment_group_name', 'type': 'category'},\n",
    "        {'name': 'detail_desc', 'type': 'text'},\n",
    "        {'name': 'customer_id', 'type': 'category'},\n",
    "        {'name': 'FN', 'type': 'category'},\n",
    "        {'name': 'Active', 'type': 'category'},\n",
    "        {'name': 'club_member_status', 'type': 'category'},\n",
    "        {'name': 'fashion_news_frequency', 'type': 'category'},\n",
    "        {'name': 'age', 'type': 'number'},\n",
    "        {'name': 'postal_code', 'type': 'category'}\n",
    "    ],\n",
    "    'output_features': [\n",
    "        {'name': 'label', 'type': 'binary', 'calibration': True}\n",
    "    ],\n",
    "    'preprocessing': {\n",
    "        'split': {\n",
    "            'type': 'fixed'\n",
    "        }\n",
    "    },\n",
    "    'combiner': {\n",
    "        'type': 'comparator',\n",
    "        'entity_1': [\n",
    "            'prod_name',\n",
    "            'product_type_name',\n",
    "            'product_group_name',\n",
    "            'graphical_appearance_name',\n",
    "            'colour_group_name',\n",
    "            'perceived_colour_value_name',\n",
    "            'perceived_colour_master_name',\n",
    "            'department_name',\n",
    "            'index_name',\n",
    "            'index_group_name',\n",
    "            'section_name',\n",
    "            'garment_group_name',\n",
    "            'detail_desc'\n",
    "        ],\n",
    "        'entity_2': [\n",
    "            'customer_id',\n",
    "            'FN',\n",
    "            'Active',\n",
    "            'club_member_status',\n",
    "            'fashion_news_frequency',\n",
    "            'age',\n",
    "            'postal_code'\n",
    "        ]\n",
    "    },\n",
    "    'trainer': {\n",
    "        'epochs': 30, \n",
    "        'batch_size': 1024, \n",
    "        'early_stop': 30\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01e677-2692-4a7a-9236-c79368a3d09c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Model Training 🏁\n",
    "\n",
    "Finally, we can kick off our model training job! With this call, we will create both a [Model Repository](https://docs.predibase.com/user-guide/models/model-repos) and train our first recsys model in that repo. As you can see, all of the pieces above have been plugged into this function call. Once you run this cell, you can click on the link to track the fine-tuning progress in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7efff-5372-4937-9e73-403b3e35124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM_recsys_model = pc.create_model(\n",
    "    repository_name=\"H&M Recommender System Model\",\n",
    "    dataset=HM_dataset,\n",
    "    config=recsys_config,\n",
    "    engine=train_engine,\n",
    "    repo_description=\"Recommend fashion products to customers\",\n",
    "    model_description=\"Baseline Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cff22a-0a8a-4ddd-8611-4e8cf1cf188c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Deploymen and Prediction 🎯\n",
    "\n",
    "Now that our model has finished training, we can deploy and start generating predictions. There are a few ways that you can generate predictions from a Predibase model: \n",
    "\n",
    "1. __REST API__ - The deployment object has an attribute called *deployment_url* which you can make an HTTP  request to in order to get predictions.\n",
    "\n",
    "2. __deployment.predict()__ - You can call `predict()` on the deployment object itself, passing in a dataframe. Just make sure to install the predibase predictor with `pip install \"predibase[predictor]\"`.\n",
    "\n",
    "3. __PQL__ - You can predict directly on data within Predibase using Predictive Query Language (PQL), our extension to SQL that allows you to run predictions with models trained in Predibase on data selected with SQL.\n",
    "\n",
    "For this example however, we will using the deployment object to run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d40af5-a356-43a0-99d6-7df84d910bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the recsys model\n",
    "deployment = pc.create_deployment(name=\"hm_recsys_deployment_1\", model=HM_recsys_model)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8866a-dc15-4f14-95d0-91bab7778f19",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "For these prediction examples, we're going to grab a random customer from the test set and generate recomendations with the products that they've interacted with in this dataset. \n",
    "\n",
    "Generally speaking, candidate products to generate recommendations with are generated with another service (collaborative filtering, content-based filtering, etc.). The output of this service becomes the input to the recsys model that we've built, which will rank the provided candidates, letting us know which ones to recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7a466-2b8d-4a2f-87c7-6d469dffea6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select random customer to generate predictions for\n",
    "random_customer_id = test_df.customer_id.sample(1).iloc[0]\n",
    "\n",
    "# Grab the set of products that this customer has interacted with - the input to our model\n",
    "prediction_data = test_df[test_df.customer_id == random_customer_id]\n",
    "\n",
    "# Run inference with the deployment object\n",
    "preds = deployment.predict(prediction_data)\n",
    "\n",
    "# Convert output vector into propensity to purchase value\n",
    "preds.label_probabilities = preds.label_probabilities.apply(lambda x: x[1])\n",
    "\n",
    "# Add propensity to purchase to product data\n",
    "prediction_data = prediction_data.join(preds[[\"label_probabilities\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190f944-d18c-417a-942c-3087980ae350",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Generate Targeted Emails 🦙 \n",
    "\n",
    "At last, we can use the recommended products to generate targeted emails for the customer based on the customer + product info. We will be using Predibase LLM capabilities to prompt LLaMa2-7B and generate these custom tailored emails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6d86f-65d5-401f-b4cf-a70dc03f3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_custom_email(product: pd.Series):\n",
    "    \"\"\"\n",
    "    Function that takes in a row of a pandas dataframe containing product and user information\n",
    "    and returns a custom tailored email advertising that product.\n",
    "    \n",
    "    :param product: A pandas Series containing product details\n",
    "    :return: Custom generated email\n",
    "    \"\"\"\n",
    "    # Extract and format product information\n",
    "    product_info = \", \".join(f\"{key}: {val}\" for key, val in product[article_cols].items())\n",
    "    \n",
    "    # Construct prompt containing product info to feed into LLM\n",
    "    prompt = f\"\"\"\n",
    "    Generate a personalized email for an outreach campaign advertising a product.\n",
    "    Product information: '{product_info}'.\n",
    "    Email:\n",
    "    \"\"\"\n",
    "    \n",
    "    return pc.prompt(prompt, \"llama-2-13b\", options={\"max_new_tokens\": 512}).response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523cdb3-3c4a-4dc7-a806-70583b736fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the top three recommendations to generate custom emails for\n",
    "top_recommendations = prediction_data.sort_values(\"label_probabilities\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4964084-5e7f-4676-aaac-45eb6ef5797c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Email to be sent out on campaign round 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31228123-4165-425c-9682-ab86f394298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subject: Elevate Your Everyday Look with Our New Ladieswear Collection\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "I hope this email finds you well. As a valued customer, we are excited to introduce our latest addition to our Womens Everyday Collection - the Cameron Blouse. This stunning piece is sure to elevate your everyday look and make you feel confident and stylish.\n",
      "\n",
      "The Cameron Blouse boasts a timeless design with a straight-cut silhouette, V-neckline, and flounces down the front that continue over the shoulders. The long sleeves with wide cuffs and covered buttons add a touch of sophistication, while the unlined construction ensures maximum comfort. Crafted from high-quality woven fabric in the perfect shade of dark black, this blouse is sure to become a wardrobe staple.\n",
      "\n",
      "As a respected customer, we would like to offer you an exclusive 15% discount on your first purchase of the Cameron Blouse. Simply use the code CAMERON15 at checkout to redeem your discount. This offer is only available for a limited time, so don't wait to elevate your wardrobe with this must-have piece.\n",
      "\n",
      "To shop our entire Ladieswear collection, please visit our website at [Your Website URL]. If you have any questions or would like to inquire about bulk orders, please do not hesitate to contact us at [Your Email Address] or [Your Phone Number]. We look forward to serving you.\n",
      "\n",
      "Thank you for your continued support, and we hope to hear from you soon.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "P.S. Don't forget to follow us on social media to stay up-to-date on the latest fashion trends and promotions.\n",
      "\n",
      "[Your Social Media Links]\n"
     ]
    }
   ],
   "source": [
    "email_1 = generate_custom_email(top_recommendations.iloc[0])\n",
    "print(email_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a04e53-896c-49a5-ab27-a006b703ae9f",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Email to be sent out on campaign round 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f75a57c7-eaa1-42fb-8858-74557c7946da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subject: 🔥 Introducing the Stunning Coronado Lace Sweater - Perfect for Your Wardrobe! 🔥\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "I hope this email finds you well! As a fashion-forward individual, I'm sure you're always on the lookout for statement pieces to elevate your wardrobe. That's why I'm thrilled to introduce you to our latest addition - the breathtaking Coronado Lace Sweater! 😍\n",
      "\n",
      "This double-knit jumper boasts a wide, lace-trimmed, ribbed V-neck, dropped shoulders, and wide sleeves with close-fitting, ribbed cuffs. The ribbed hem adds a touch of elegance, making it perfect for both casual and dressy occasions. The Black color with a perceived Dark value is sure to turn heads! 💥\n",
      "\n",
      "As part of our Divided Collection, this sweater is a must-have for any fashion-conscious individual. The Garment Upper body category ensures a comfortable fit, while the Lace graphical appearance adds a touch of sophistication. Whether you're dressing up or dressing down, this sweater is sure to impress! 💃\n",
      "\n",
      "Here are some key features of the Coronado Lace Sweater:\n",
      "\n",
      "✨ Double-knit jumper with a wide, lace-trimmed, ribbed V-neck\n",
      "✨ Dropped shoulders and wide sleeves with close-fitting, ribbed cuffs\n",
      "✨ Ribbed hem for added elegance\n",
      "✨ Black color with a perceived Dark value\n",
      "✨ Part of our Divided Collection, Garment Upper body category, and Lace graphical appearance\n",
      "\n",
      "We're confident that the Coronado Lace Sweater will become your new go-to piece this season! Don't miss out on the opportunity to elevate your wardrobe with this stunning sweater. Order now and experience the comfort and style it offers! 💕\n",
      "\n",
      "To purchase, simply click on the link below or visit our website:\n",
      "\n",
      "[Insert link]\n",
      "\n",
      "If you have any questions or would like to learn more\n"
     ]
    }
   ],
   "source": [
    "email_2 = generate_custom_email(top_recommendations.iloc[1])\n",
    "print(email_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf11275-ebb2-4701-9657-874c3dfddc37",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Email to be sent out on campaign round 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05107c72-ca57-4dc3-9a72-4967c094bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subject: Introducing the Burmilla Blazer - Elevate Your Wardrobe with Timeless Elegance\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "I hope this email finds you well. As a valued professional, you understand the importance of dressing for success. That's why I'm excited to introduce you to our latest addition - the Burmilla Blazer. This exquisite piece is sure to elevate your wardrobe and make a lasting impression.\n",
      "\n",
      "The Burmilla Blazer is a straight-cut jacket crafted from high-quality woven fabric, featuring notch lapels, jetted front pockets, and 3/4-length sleeves with gathers at the cuffs. Its timeless design ensures that it will remain a staple in your wardrobe for years to come.\n",
      "\n",
      "Here are some key features that set the Burmilla Blazer apart:\n",
      "\n",
      "* Solid, dark color that exudes professionalism and sophistication\n",
      "* Made from premium materials for durability and longevity\n",
      "* Straight-cut design for a flattering fit\n",
      "* Notch lapels and jetted front pockets add a touch of elegance\n",
      "* 3/4-length sleeves with gathers at the cuffs provide a feminine touch\n",
      "* Lined for a smooth, comfortable fit\n",
      "\n",
      "Whether you're attending a business meeting, networking event, or special occasion, the Burmilla Blazer is the perfect choice. Its versatility and timeless design make it suitable for a wide range of settings, ensuring that you look and feel confident and professional.\n",
      "\n",
      "We offer a wide range of sizes to fit every body type, and our friendly customer service team is always available to assist with any questions or concerns. Plus, with our easy return and exchange policy, you can shop with confidence.\n",
      "\n",
      "Don't miss out on the opportunity to elevate your wardrobe with the Burmilla Blazer. Order now and experience the difference that timeless elegance can make in your professional and personal life.\n",
      "\n",
      "To place an order or learn more about our products, please visit our website or contact us at [Your Company's Contact Information]. We look forward to serving you.\n",
      "\n",
      "Thank you for considering our brand.\n",
      "\n",
      "Best regards,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_3 = generate_custom_email(top_recommendations.iloc[2])\n",
    "print(email_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3be6c9-4dbe-45c4-88d0-e26c1b6eb5ab",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "And there you have it, an end to end tutorial on how to set up a recommender system model in Predibase, and chain the outputs of this model with Predibase's LLM capabilities to generate custom tailored emails for the recommended products.\n",
    "\n",
    "When it comes to the generation step, there is actually a lot of control you have over the generated email via the prompt you pass in. As you saw in the `generate_custom_email` function, we were using a pretty generic prompt where we just pass in the product info and ask the LLM to generate an advertising email. However, you can try all sorts of things with the prompt such as adjusting, tone, length, or even passing in user information to put into the email if you have it available.\n",
    "\n",
    "The purpose of this tutorial was to provide a basic guideline for building solutions of this nature. In addition to this tutorial notebook, we also have a sample application that we've generated to showcase what a simple application built with this type of tooling could look like. We hope you enjoyed this tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39fe7bc-383e-4014-a10d-f5cd686e2e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predibase38",
   "language": "python",
   "name": "predibase38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
