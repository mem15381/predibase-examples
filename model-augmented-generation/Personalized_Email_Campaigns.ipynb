{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e716cd-5d42-4816-a666-279541aca3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from predibase import PredibaseClient\n",
    "\n",
    "from ludwig.data.split import get_splitter\n",
    "from ludwig.data.negative_sampling import negative_sample\n",
    "from ludwig.backend.base import LocalBackend\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbe747-5965-4748-afd6-e0ea3a1e06d8",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "# Personalized Email Campaings 📨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025b901-6650-4745-9262-cc7e3876ce41",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Recommender System Model --> LLM Generated Personalized Email Campaigns__\n",
    "\n",
    "In this tutorial, we show how to use the Predibase SDK to train a recommender system (recsys) model, then take the results from the recsys model and generate personalized outreach emails via generations from open-source LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c1dff-6499-4c5c-a637-2faab6fd2c87",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Authentication 🔐\n",
    "\n",
    "The first step is to sign into Predibase.\n",
    "\n",
    "- If you do not have a Predibase account set up yet, you may sign up for a free account [here](https://predibase.com/free-trial)\n",
    "- If you already have an account, navigate to Settings -> My Profile and generate a new API token.\n",
    "- Finally, plug in the generated API token in the code below to authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316e586-a8d2-4b3a-b13f-385971ea0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PredibaseClient(\n",
    "    token=\"YOUR TOKEN HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5feb6e-7e11-48a8-9d68-bd7f80c5de59",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "## Dataset Preparation 📄\n",
    "\n",
    "Next we'll prepare the dataset needed to train the model the recsys model. For this demonstration, we will be using the [H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data)  dataset from Kaggle. For this tutorial, we do not need the images, which are the largest part of the download, so we recommend individually downloading `articles.csv`, `customers.csv`, and `transactions_train.csv` to minimize the download process time. This dataset contains the purchase history of customers across time, along with supporting metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6694a76c-b7e3-4186-a98a-bac494c97d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(\"PATH TO articles.csv DOWNLOAD\")\n",
    "customers_df = pd.read_csv(\"PATH TO customers.csv DOWNLOAD\")\n",
    "transactions_df = pd.read_csv(\"PATH TO transactions_train.csv DOWNLOAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f44f2-8252-4639-8e00-de0f1bb82b47",
   "metadata": {},
   "source": [
    "<br/>\n",
    "First we're going to merge the three separate dataframes together to form a single transaction dataframe. This dataframe will contain all transactions that took place by the users in the dataset. Because every row is a transaction that took place, we need to add in some examples of transactions that didn't take place - we call these negative samples. We will handle that a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a1c26c-a6ab-462d-8e67-e8bf4e879163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the transactions and articles dataframes\n",
    "transactions_df = pd.merge(\n",
    "    transactions_df,\n",
    "    articles_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"article_id\",\n",
    "    right_on=\"article_id\",\n",
    ")\n",
    "\n",
    "# Merge the transactions and customers dataframes\n",
    "transactions_df = pd.merge(\n",
    "    transactions_df,\n",
    "    customers_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"customer_id\",\n",
    "    right_on=\"customer_id\",\n",
    ")\n",
    "\n",
    "# Set label to 1 for all known transactions, since the customer bought the article\n",
    "transactions_df[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829522ba-447b-40cb-84a0-e4fb3ee66fa1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Now that we have a single dataframe with all transaction, article, and customer data present in every row, we are going to both sample and split the dataset. \n",
    "\n",
    "#### Sampling\n",
    "We sample the dataset here because of scale. The `transactions_df` has ~31.8 million rows and ~1.3 million unique customers. This is great for training a very strong model, however, processing this data will take considerably longer. For the purposes of this tutorial, we are sampling in the following ways:\n",
    "- Sample from transactions that occured after August 21st, 2020\n",
    "- Sample from customers who purchased 10 or more items after August 21st, 2020\n",
    "- Select 500 samples\n",
    "\n",
    "\n",
    "#### Split\n",
    "We need to split our data into a training, validation, and test set. In addition to this though, we also need to split in a way that makes sure a given customer's transaction are present in each set. This way, the learning that takes place in the training set can be fairly evaluated in the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f25b6e-9a96-4a45-8272-68ba4bb50107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transaction date column to datetime object\n",
    "transactions_df[\"t_dat\"] = pd.to_datetime(transactions_df.t_dat)\n",
    "\n",
    "# Strip out year and month for splitting/sampling purposes\n",
    "transactions_df[\"year_month\"] = transactions_df.t_dat.dt.to_period(\"M\").dt.strftime(\"%Y-%m\")\n",
    "\n",
    "# Slice dataset to keep everything after \"2020-08-21\"\n",
    "sampled_transactions_df = transactions_df[transactions_df.t_dat > \"2020-08-21\"]\n",
    "\n",
    "# Sample 500 customers with 10 or more transactions, since ~1.3 million total customers takes quite a while to process\n",
    "customer_ids = np.random.choice(sampled_transactions_df.groupby(\"customer_id\").filter(lambda x: len(x) >= 10).customer_id, 500, replace=False)\n",
    "sampled_transactions_df = sampled_transactions_df[sampled_transactions_df.customer_id.isin(customer_ids)]\n",
    "\n",
    "# Get ludwig datetime splitter to ensure no target leakage by date. Split 70%, 20%, 10% for train, validation, and test sets\n",
    "splitter = get_splitter(\"datetime\", column=\"year_month\", probabilities=(0.7, 0.2, 0.1))\n",
    "\n",
    "# Split per customer_id to ensure that interactions for a customer are across all splits\n",
    "train_dfs, val_dfs, test_dfs = [], [], []\n",
    "for customer_id in sampled_transactions_df[\"customer_id\"].unique():\n",
    "    train_df, val_df, test_df = splitter.split(sampled_transactions_df[sampled_transactions_df[\"customer_id\"] == customer_id], backend=LocalBackend())\n",
    "    \n",
    "    train_dfs.append(train_df)\n",
    "    val_dfs.append(val_df)\n",
    "    test_dfs.append(test_df)\n",
    "    \n",
    "# Concatenate all customer id specific splits into their respective datasets\n",
    "train_set = pd.concat(train_dfs)\n",
    "val_set = pd.concat(val_dfs)\n",
    "test_set = pd.concat(test_dfs)\n",
    "\n",
    "# Set the split value for each set\n",
    "train_set[\"split\"] = 0\n",
    "val_set[\"split\"] = 1\n",
    "test_set[\"split\"] = 2\n",
    "\n",
    "# Combine train, val, and test set into final dataset\n",
    "full_df = pd.concat([train_set, val_set, test_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17797d3-c7d8-48bc-a4bf-128feedaeac7",
   "metadata": {},
   "source": [
    "<br/>\n",
    "As mentioned before, we need to add negative samples so that the model can learn to distinguish between something a user would buy, and something a user would not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86debc3-b33d-4772-b857-64b88dcf6060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sample each split separately\n",
    "train_df = negative_sample(full_df[full_df.split == 0], neg_pos_ratio=10, neg_val=0)\n",
    "val_df = negative_sample(full_df[full_df.split == 1], neg_pos_ratio=10, neg_val=0)\n",
    "test_df = negative_sample(full_df[full_df.split == 2], neg_pos_ratio=10, neg_val=0)\n",
    "\n",
    "# The negative_sample utility from Ludwig only returns a user_id, item_id, and label column. So\n",
    "# we need to define the columns we want to add back into the dataset.\n",
    "article_cols = [\n",
    "    \"prod_name\",\n",
    "    \"product_type_name\",\n",
    "    \"product_group_name\",\n",
    "    \"graphical_appearance_name\",\n",
    "    \"colour_group_name\",\n",
    "    \"perceived_colour_value_name\",\n",
    "    \"perceived_colour_master_name\",\n",
    "    \"department_name\",\n",
    "    \"index_name\",\n",
    "    \"index_group_name\",\n",
    "    \"section_name\",\n",
    "    \"garment_group_name\",\n",
    "    \"detail_desc\",\n",
    "]\n",
    "customer_cols = [\n",
    "    \"customer_id\",\n",
    "    \"FN\",\n",
    "    \"Active\",\n",
    "    \"club_member_status\",\n",
    "    \"fashion_news_frequency\",\n",
    "    \"age\",\n",
    "    \"postal_code\",\n",
    "]\n",
    "\n",
    "# Add back customer and article features defined above\n",
    "articles = full_df[[\"article_id\"] + article_cols].drop_duplicates([\"article_id\"])\n",
    "customers = full_df[customer_cols].drop_duplicates([\"customer_id\"])\n",
    "\n",
    "train_df = pd.merge(train_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
    "train_df = pd.merge(train_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
    "train_df[\"split\"] = 0\n",
    "\n",
    "val_df = pd.merge(val_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
    "val_df = pd.merge(val_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
    "val_df[\"split\"] = 1\n",
    "\n",
    "test_df = pd.merge(test_df, articles, how=\"left\", left_on=\"article_id\", right_on=\"article_id\")\n",
    "test_df = pd.merge(test_df, customers, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\n",
    "test_df[\"split\"] = 2\n",
    "\n",
    "# Combine train, val, and test sets together to get the final dataset we will train the model with.\n",
    "final_df = pd.concat([train_df, val_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b1f0e-919a-4f4b-8e30-ffb10eb12360",
   "metadata": {},
   "source": [
    "<br/> \n",
    "\n",
    "__NOTE:__ Because we decided to sample the dataset to a fraction of it's size, the file size of our dataset is under the 1GB limit that Predibase has on file uploads. So for the purposes of this example, we will go straight from a dataframe to a Predibase dataset. However, for production use cases, it is recommended that you use an object storage such as AWS S3 to hold your dataset artifacts. This way you can connect and train on datasets much larger than 1GB in Predibase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b108ea-85ca-4bb3-bde1-25a1a59addd9",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Connect Data ♾️\n",
    "\n",
    "Here we are using the [`create_dataset_from_df`](https://docs.staging.predibase.com/sdk-guide/datasets/dataset_from_df) method which creates a converts a pandas dataframe to a Predibase File Upload Dataset. It is likely that you are using another connection option and should use a different method of creating a Predibase Dataset. We have all the [connection](https://docs.staging.predibase.com/sdk-guide/connections/) and [dataset](https://docs.staging.predibase.com/sdk-guide/datasets/) options available to reference in the [SDK docs](https://docs.staging.predibase.com/sdk-guide/getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb5be08-186f-4bd2-9da3-93251d6e8585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id=2579, name=H&M_Recsys_Dataset, object_name=c925cbfa6bc64315a26ed8559f0c398a, connection_id=3, author=connor_mccormick_predibase, created=2023-08-31T01:05:23.955409Z, updated=2023-08-31T01:05:23.955409Z)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the dataset to dataframe SDK method to create a Predibase dataset from our final df above.\n",
    "HM_dataset = pc.create_dataset_from_df(final_df, \"H&M_Recsys_Dataset\")\n",
    "HM_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55683865-5518-46ad-b076-59d2b4869ef3",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Engine 🚂\n",
    "\n",
    "At Predibase, engines are our solution to common compute and infrastructure pain points that everyone runs into while training models. These are problems like:\n",
    "- Encountering Out of Memory errors due to insufficient compute\n",
    "- Challenges distributing a model training job over multiple compute resources\n",
    "- Losing progress when transient issues interrupt the training process\n",
    "\n",
    "Predibase training engines mitigate these issues by:\n",
    "- Analyzing the training job details to assign the right amount of compute\n",
    "- Logic to distribute the training job over the assigned compute resources\n",
    "- Retry logic when things go wrong\n",
    "\n",
    "With this in mind, we will select the engine we want to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff04d9f3-db89-40f7-aec9-6b85c995b2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(id=50, name=train_engine, status=suspended, type=kubernetes, service_type=batch, template_id=8, environment_id=1, auto_suspend_seconds=0, auto_resume=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_engine = pc.get_engine(\"train_engine\")\n",
    "train_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b51795-764d-4524-83a6-2402728eb4d2",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Config 📝\n",
    "\n",
    "Next, we're going to define the config with the specs for our recommender systems model. Here is a readable yaml representation of the config - below I will explain the key parameters we're setting and why.\n",
    "```\n",
    "model_type: ecd\n",
    "input_features:\n",
    "  - name: prod_name\n",
    "    type: text\n",
    "  - name: product_type_name\n",
    "    type: category\n",
    "  - name: product_group_name\n",
    "    type: category\n",
    "  - name: graphical_appearance_name\n",
    "    type: category\n",
    "  - name: colour_group_name\n",
    "    type: category\n",
    "  - name: perceived_colour_value_name\n",
    "    type: category\n",
    "  - name: perceived_colour_master_name\n",
    "    type: category\n",
    "  - name: department_name\n",
    "    type: category\n",
    "  - name: index_name\n",
    "    type: category\n",
    "  - name: index_group_name\n",
    "    type: category\n",
    "  - name: section_name\n",
    "    type: category\n",
    "  - name: garment_group_name\n",
    "    type: category\n",
    "  - name: detail_desc\n",
    "    type: text\n",
    "  - name: customer_id\n",
    "    type: category\n",
    "  - name: FN\n",
    "    type: category\n",
    "  - name: Active\n",
    "    type: category\n",
    "  - name: club_member_status\n",
    "    type: category\n",
    "  - name: fashion_news_frequency\n",
    "    type: category\n",
    "  - name: age\n",
    "    type: number\n",
    "  - name: postal_code\n",
    "    type: category\n",
    "output_features:\n",
    "  - name: label\n",
    "    type: binary\n",
    "    calibration: true\n",
    "preprocessing:\n",
    "  split:\n",
    "    type: fixed\n",
    "combiner:\n",
    "  type: comparator\n",
    "  entity_1:\n",
    "    - prod_name\n",
    "    - product_type_name\n",
    "    - product_group_name\n",
    "    - graphical_appearance_name\n",
    "    - colour_group_name\n",
    "    - perceived_colour_value_name\n",
    "    - perceived_colour_master_name\n",
    "    - department_name\n",
    "    - index_name\n",
    "    - index_group_name\n",
    "    - section_name\n",
    "    - garment_group_name\n",
    "    - detail_desc\n",
    "  entity_2:\n",
    "    - customer_id\n",
    "    - FN\n",
    "    - Active\n",
    "    - club_member_status\n",
    "    - fashion_news_frequency\n",
    "    - age\n",
    "    - postal_code\n",
    "trainer:\n",
    "  epochs: 30\n",
    "  batch_size: 1024\n",
    "  early_stop: 30\n",
    "```\n",
    "A few of the key parameters set are outlined below:\n",
    "- `output_features.calibration`: \n",
    "- `preprocessing.split.type`:\n",
    "- `combiner.type`:\n",
    "- `combiner.entity_1`:\n",
    "- `combiner.entity_2`:\n",
    "\n",
    "For more configuration details, check out the [Ludwig LLM Docs](https://ludwig.ai/0.8/configuration/large_language_model/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eebcabd4-c7b9-42f5-bbca-96523e839437",
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys_config = {\n",
    "    'model_type': 'ecd',\n",
    "    'input_features': [\n",
    "        {'name': 'prod_name', 'type': 'text'},\n",
    "        {'name': 'product_type_name', 'type': 'category'},\n",
    "        {'name': 'product_group_name', 'type': 'category'},\n",
    "        {'name': 'graphical_appearance_name', 'type': 'category'},\n",
    "        {'name': 'colour_group_name', 'type': 'category'},\n",
    "        {'name': 'perceived_colour_value_name', 'type': 'category'},\n",
    "        {'name': 'perceived_colour_master_name', 'type': 'category'},\n",
    "        {'name': 'department_name', 'type': 'category'},\n",
    "        {'name': 'index_name', 'type': 'category'},\n",
    "        {'name': 'index_group_name', 'type': 'category'},\n",
    "        {'name': 'section_name', 'type': 'category'},\n",
    "        {'name': 'garment_group_name', 'type': 'category'},\n",
    "        {'name': 'detail_desc', 'type': 'text'},\n",
    "        {'name': 'customer_id', 'type': 'category'},\n",
    "        {'name': 'FN', 'type': 'category'},\n",
    "        {'name': 'Active', 'type': 'category'},\n",
    "        {'name': 'club_member_status', 'type': 'category'},\n",
    "        {'name': 'fashion_news_frequency', 'type': 'category'},\n",
    "        {'name': 'age', 'type': 'number'},\n",
    "        {'name': 'postal_code', 'type': 'category'}\n",
    "    ],\n",
    "    'output_features': [\n",
    "        {'name': 'label', 'type': 'binary', 'calibration': True}\n",
    "    ],\n",
    "    'preprocessing': {\n",
    "        'split': {\n",
    "            'type': 'fixed'\n",
    "        }\n",
    "    },\n",
    "    'combiner': {\n",
    "        'type': 'comparator',\n",
    "        'entity_1': [\n",
    "            'prod_name',\n",
    "            'product_type_name',\n",
    "            'product_group_name',\n",
    "            'graphical_appearance_name',\n",
    "            'colour_group_name',\n",
    "            'perceived_colour_value_name',\n",
    "            'perceived_colour_master_name',\n",
    "            'department_name',\n",
    "            'index_name',\n",
    "            'index_group_name',\n",
    "            'section_name',\n",
    "            'garment_group_name',\n",
    "            'detail_desc'\n",
    "        ],\n",
    "        'entity_2': [\n",
    "            'customer_id',\n",
    "            'FN',\n",
    "            'Active',\n",
    "            'club_member_status',\n",
    "            'fashion_news_frequency',\n",
    "            'age',\n",
    "            'postal_code'\n",
    "        ]\n",
    "    },\n",
    "    'trainer': {\n",
    "        'epochs': 30, \n",
    "        'batch_size': 1024, \n",
    "        'early_stop': 30\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01e677-2692-4a7a-9236-c79368a3d09c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Model Training 🏁\n",
    "\n",
    "Finally, we can kick off our model training job! With this call, we will create both a [Model Repository](https://docs.predibase.com/user-guide/models/model-repos) and train our first recsys model in that repo. As you can see, all of the pieces above have been plugged into this function call. Once you run this cell, you can click on the link to track the fine-tuning progress in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8f7efff-5372-4937-9e73-403b3e35124c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Created model repository: &lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">H</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&amp;M Recommender System Model</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mCreated model repository: \u001b[0m\u001b[1;34m<\u001b[0m\u001b[1;95mH\u001b[0m\u001b[1;39m&M Recommender System Model\u001b[0m\u001b[1;34m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Training model version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1511</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> for model repository &lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">H</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&amp;M Recommender System Model</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&gt;</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mTraining model version \u001b[0m\u001b[1;36m1511\u001b[0m\u001b[1;34m for model repository \u001b[0m\u001b[1;34m<\u001b[0m\u001b[1;95mH\u001b[0m\u001b[1;39m&M Recommender System Model\u001b[0m\u001b[1;34m>\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Check Status of Model Training Here: </span><a href=\"http://localhost:8000/models/version/3681\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8000/models/version/3681</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mCheck Status of Model Training Here: \u001b[0m\u001b]8;id=657967;http://localhost:8000/models/version/3681\u001b\\\u001b[4;94mhttp://localhost:8000/models/version/3681\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HM_recsys_model = pc.create_model(\n",
    "    repository_name=\"H&M Recommender System Model\",\n",
    "    dataset=HM_dataset,\n",
    "    config=recsys_config,\n",
    "    engine=train_engine,\n",
    "    repo_description=\"Recommend fashion products to customers\",\n",
    "    model_description=\"Baseline Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cff22a-0a8a-4ddd-8611-4e8cf1cf188c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Deploymen and Prediction 🎯\n",
    "\n",
    "Now that our model has finished training, we can deploy and start generating predictions. There are a few ways that you can generate predictions from a Predibase model: \n",
    "\n",
    "1. __REST API__ - The deployment object has an attribute called *deployment_url* which you can make an HTTP  request to in order to get predictions.\n",
    "\n",
    "2. __deployment.predict()__ - You can call `predict()` on the deployment object itself, passing in a dataframe. Just make sure to install the predibase predictor with `pip install \"predibase[predictor]\"`.\n",
    "\n",
    "3. __PQL__ - You can predict directly on data within Predibase using Predictive Query Language (PQL), our extension to SQL that allows you to run predictions with models trained in Predibase on data selected with SQL.\n",
    "\n",
    "For this example however, we will using the deployment object to run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d40af5-a356-43a0-99d6-7df84d910bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██</span> Create Deployment...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██\u001b[0m Create Deployment...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Create Deployment\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Create Deployment\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deploy the recsys model\n",
    "deployment = pc.create_deployment(name=\"hm_recsys_deployment_1\", model=HM_recsys_model)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8866a-dc15-4f14-95d0-91bab7778f19",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "For these prediction examples, we're going to grab a random customer from the test set and generate recomendations with the products that they've interacted with in this dataset. \n",
    "\n",
    "Generally speaking, candidate products to generate recommendations with are generated with another service (collaborative filtering, content-based filtering, etc.). The output of this service becomes the input to the recsys model that we've built, which will rank the provided candidates, letting us know which ones to recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d7a466-2b8d-4a2f-87c7-6d469dffea6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">█████████▁▁▁▁▁▁▁▁▁▁▁</span> Predict...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m█████████▁▁▁▁▁▁▁▁▁▁▁\u001b[0m Predict...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Predict\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Predict\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select random customer to generate predictions for\n",
    "random_customer_id = test_df.customer_id.sample(1).iloc[0]\n",
    "\n",
    "# Grab the set of products that this customer has interacted with - the input to our model\n",
    "prediction_data = test_df[test_df.customer_id == random_customer_id]\n",
    "\n",
    "# Run inference with the deployment object\n",
    "preds = deployment.predict(prediction_data)\n",
    "\n",
    "# Convert output vector into propensity to purchase value\n",
    "preds.label_probabilities = preds.label_probabilities.apply(lambda x: x[1])\n",
    "\n",
    "# Add propensity to purchase to product data\n",
    "prediction_data = prediction_data.join(preds[[\"label_probabilities\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190f944-d18c-417a-942c-3087980ae350",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Generate Targeted Emails 🦙 \n",
    "\n",
    "At last, we can use the recommended products to generate targeted emails for the customer based on the customer + product info. We will be using Predibase LLM capabilities to prompt LLaMa2-7B and generate these custom tailored emails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1e6d86f-65d5-401f-b4cf-a70dc03f3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_custom_email(product: pd.Series):\n",
    "    \"\"\"\n",
    "    Function that takes in a row of a pandas dataframe containing product and user information\n",
    "    and returns a custom tailored email advertising that product.\n",
    "    \n",
    "    :param product: A pandas Series containing product details\n",
    "    :return: Custom generated email\n",
    "    \"\"\"\n",
    "    # Extract and format product information\n",
    "    product_info = \", \".join(f\"{key}: {val}\" for key, val in product[article_cols].items())\n",
    "    \n",
    "    # Construct prompt containing product info to feed into LLM\n",
    "    prompt = f\"\"\"\n",
    "    Generate a personalized email for an outreach campaign advertising a product.\n",
    "    Product information: '{product_info}'.\n",
    "    Email:\n",
    "    \"\"\"\n",
    "    \n",
    "    return pc.prompt(prompt, \"llama-2-13b\", options={\"max_new_tokens\": 512}).response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2523cdb3-3c4a-4dc7-a806-70583b736fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the top three recommendations to generate custom emails for\n",
    "top_recommendations = prediction_data.sort_values(\"label_probabilities\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4964084-5e7f-4676-aaac-45eb6ef5797c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Email to be sent out on campaign round 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31228123-4165-425c-9682-ab86f394298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subject: Introducing the Perfect Pair of Trousers for Your Everyday Look - Pluto RW Slacks\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "I hope this email finds you well. As a fashion-forward professional like yourself, I'm sure you're always on the lookout for the perfect pair of trousers to complement your everyday look. That's why I'm excited to introduce you to our latest addition to the Womens Everyday Collection - the Pluto RW Slacks.\n",
      "\n",
      "Our Pluto RW Slacks are designed to provide you with the ultimate combination of style, comfort, and versatility. Crafted from a stretch weave with a zip fly and concealed hook-and-eye fastening, these ankle-length cigarette trousers are tailored to flatter your figure and provide a comfortable fit. The regular waist with concealed elastication ensures a perfect fit, while the side pockets, fake back pockets, and tapered legs add a stylish touch.\n",
      "\n",
      "The Pluto RW Slacks are part of our Ladieswear collection, and they come in the perfect shade of Dark Black, which is sure to complement any outfit. Whether you're dressing up or dressing down, these trousers are the perfect choice for any professional setting.\n",
      "\n",
      "Here are some key features of the Pluto RW Slacks:\n",
      "\n",
      "* Product Name: Pluto RW Slacks\n",
      "* Product Type Name: Trousers\n",
      "* Product Group Name: Garment Lower Body\n",
      "* Graphical Appearance Name: Solid\n",
      "* Colour Group Name: Black\n",
      "* Perceived Colour Value Name: Dark\n",
      "* Perceived Colour Master Name: Black\n",
      "* Department Name: Trouser\n",
      "* Index Name: Ladieswear\n",
      "* Index Group Name: Ladieswear\n",
      "* Section Name: Womens Everyday Collection\n",
      "* Garment Group Name: Trousers\n",
      "* Detail Description: Ankle-length cigarette trousers in a stretch weave with a zip fly, concealed hook-and-eye fastening, and regular waist with concealed elastication. Side pockets, fake back pockets, and tapered legs.\n",
      "\n",
      "We're confident\n"
     ]
    }
   ],
   "source": [
    "email_1 = generate_custom_email(top_recommendations.iloc[0])\n",
    "print(email_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a04e53-896c-49a5-ab27-a006b703ae9f",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Email to be sent out on campaign round 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f75a57c7-eaa1-42fb-8858-74557c7946da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subject: 👗 Introducing the Perfect Pair of Trousers for Your Everyday Wardrobe - Pluto RW Slacks 👖\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "We hope this email finds you well! As a fashion-forward individual, we're sure you're always on the lookout for the latest must-have pieces to elevate your wardrobe. That's why we're thrilled to introduce you to our latest addition - Pluto RW Slacks! 😍\n",
      "\n",
      "These ankle-length cigarette trousers are a game-changer for your everyday look. Crafted from a stretchy weave with a zip fly, concealed hook-and-eye fastening, and regular waist with concealed elastication, these trousers offer unparalleled comfort and a perfect fit. The tapered legs and side pockets add a stylish touch, while the fake back pockets provide a chic finish.\n",
      "\n",
      "Pluto RW Slacks are part of our Womens Everyday Collection, specifically in the Ladieswear section, and are categorized under the Garment Lower Body department. Their solid black color with a dark perceived color value makes them versatile and easy to pair with any top or shoes. Plus, they're made from a high-quality stretch material that will keep you comfortable all day long!\n",
      "\n",
      "We're confident that Pluto RW Slacks will become your new go-to trousers for work, a night out, or any casual occasion. Here's why:\n",
      "\n",
      "🔹 Perfect fit: With a stretchy weave and concealed elastication, these trousers hug your legs for a comfortable fit that lasts all day.\n",
      "\n",
      "🔹 Versatile style: The solid black color and tapered legs make these trousers easy to pair with any top or shoes, perfect for work or play.\n",
      "\n",
      "🔹 High-quality material: Made from a premium stretch material that's both comfortable and durable, ensuring you'll wear them again and again.\n",
      "\n",
      "We're offering an exclusive 15% discount for first-time buyers! Simply use the code PLUTO\n"
     ]
    }
   ],
   "source": [
    "email_2 = generate_custom_email(top_recommendations.iloc[1])\n",
    "print(email_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf11275-ebb2-4701-9657-874c3dfddc37",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Email to be sent out on campaign round 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05107c72-ca57-4dc3-9a72-4967c094bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subject: Introducing the Pluto RW Slacks - Elevate Your Everyday Look!\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "I hope this email finds you well. As a valued professional, you understand the importance of dressing for success. That's why we're excited to introduce our latest addition to the Womens Everyday Collection - the Pluto RW Slacks! 👖\n",
      "\n",
      "These ankle-length cigarette trousers are crafted from a stretchy weave, providing a comfortable fit and a sleek silhouette. With a zip fly, concealed hook-and-eye fastening, and regular waist with concealed elastication, these pants are not only stylish but also practical. The side pockets, fake back pockets, and tapered legs complete the look, making them perfect for any professional setting.\n",
      "\n",
      "The Pluto RW Slacks are part of our Ladieswear collection, and they come in the versatile and timeless color, Black. The perceived color value is Dark, and the product group name is Garment Lower Body. Whether you're dressing up or dressing down, these pants are sure to elevate your everyday look!\n",
      "\n",
      "We believe that these pants would be a great addition to your wardrobe, and we would love for you to experience the comfort and style they offer. As a valued recipient of this email, we would like to extend an exclusive offer of [insert offer here, e.g., 15% off your first purchase].\n",
      "\n",
      "To shop our collection, simply click on the link below:\n",
      "\n",
      "[Insert link here]\n",
      "\n",
      "If you have any questions or would like to learn more about our products, please don't hesitate to reach out. We're here to help!\n",
      "\n",
      "Thank you for your time, and we look forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "P.S. Don't forget to follow us on social media to stay up-to-date on the latest fashion trends and promotions! 💕\n",
      "\n",
      "[Your Social Media Links]\n"
     ]
    }
   ],
   "source": [
    "email_3 = generate_custom_email(top_recommendations.iloc[2])\n",
    "print(email_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3be6c9-4dbe-45c4-88d0-e26c1b6eb5ab",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "And there you have it, an end to end tutorial on how to set up a recommender system model in Predibase, and chain the outputs of this model with Predibase's LLM capabilities to generate custom tailored emails for the recommended products.\n",
    "\n",
    "When it comes to the generation step, there is actually a lot of control you have over the generated email via the prompt you pass in. As you saw in the `generate_custom_email` function, we were using a pretty generic prompt where we just pass in the product info and ask the LLM to generate an advertising email. However, you can try all sorts of things with the prompt such as adjusting, tone, length, or even passing in user information to put into the email if you have it available.\n",
    "\n",
    "The purpose of this tutorial was to provide a basic guideline for building solutions of this nature. In addition to this tutorial notebook, we also have a [sample application](https://github.com/predibase/examples/tree/main/model-augmented-generation) that we've generated to showcase what a simple application built with this type of tooling could look like. We hope you enjoyed this tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predibase38",
   "language": "python",
   "name": "predibase38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
